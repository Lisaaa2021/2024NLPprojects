{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1CCdxscdYZpsif2KXHa3a5KaIhJPaThOY","authorship_tag":"ABX9TyNSfkreeK1IQ+AlTR324npH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6SE6TFLoVUN4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714391722533,"user_tz":-120,"elapsed":6,"user":{"displayName":"S W","userId":"09872337056058742244"}},"outputId":"2e272841-ebd6-4ccd-8431-ca8058bfc5d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1yGH_vD7fWnFThhvEZlRCOlvmeHWALf8x/Contextualized_embedding\n"]}],"source":["'cd /content/drive/MyDrive/2024_UvA/New_pipeline_Contextualized_embedding_BERT"]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfDiy_UVfATh","executionInfo":{"status":"ok","timestamp":1714391437832,"user_tz":-120,"elapsed":330,"user":{"displayName":"S W","userId":"09872337056058742244"}},"outputId":"26ab8b82-e400-4bf2-9dde-ef1dbadd7e5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["notebook.ipynb  output_2.txt  st_df.pkl  Variable_under_divided_government_or_NOT.csv\n","output_1.txt    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  util.py\n"]}]},{"cell_type":"markdown","source":["# Project Readme\n","*   This project adapted code from this post: https://medium.com/@r3d_robot/getting-contextualized-word-embeddings-with-bert-20798d8b43a4\n","*   This pipeline will skip sentences longer than 512 tokens\n","*   This pipeline takes the sum of the 4 four layers as the contextualized embedding\n","\n"],"metadata":{"id":"BupmJa3liDD7"}},{"cell_type":"code","source":["from util import output_two_corpus, get_keyword_corpus,get_contextulized_embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKRAHTiVj-Ue","executionInfo":{"status":"ok","timestamp":1714391782722,"user_tz":-120,"elapsed":5835,"user":{"displayName":"S W","userId":"09872337056058742244"}},"outputId":"565487a5-4b05-4289-9ee1-ce30d7daef03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["keyword = 'postabortion'\n","\n","corpus_1, corpus_2 = output_two_corpus()\n","#This function takes a variable_file_path parameter, which contains variable(s) that will be used to group the DEC corpus.\\\n","#The file 'st_df.pk1' contains a pre-processed dataframe of the DEC corpus.\\\n","#Output: two divided corpus lists with each containing instances at the sentence level for the next step."],"metadata":{"id":"AtSqpxyniCMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword_corpus_1, keyword_corpus_2 = get_keyword_corpus(corpus_1, corpus_2, keyword)\n","#use postabortion as an example. BERT tokenizer splits it into 'post + ab + ort + tion'\n","\n","#Input: two corpus and the keyword\\\n","#Output two list type corpus list with sentences containing the keyword"],"metadata":{"id":"qMC6HGArrPFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword_embeddings = get_contextulized_embedding(keyword_corpus_1, keyword)\n","\n","#Input: a corpus output from get_keyword_corpus and the keyword\\\n","#Output: keyword embeddings\n"],"metadata":{"id":"IcBqH34XbIkk"},"execution_count":null,"outputs":[]}]}